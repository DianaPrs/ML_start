{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rnn.png](rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recurrent neural network \n",
    "- without math https://www.youtube.com/watch?v=LHXXI4-IEns\n",
    "- with math https://www.youtube.com/watch?v=ySEx_Bqxvvo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "RNN ~ 1986 year\n",
    "\n",
    "\n",
    "LSTM ~ 1997 year\n",
    "\n",
    "\n",
    "Transformers ~ 2017 year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM:\n",
    "\n",
    "(3 min explanation) https://www.youtube.com/watch?v=5dMXyiWddYs\n",
    "Long short-term memory (LSTM) is a type of recurrent neural network (RNN) aimed at mitigating the vanishing gradient problem commonly encountered by traditional RNNs. \n",
    "\n",
    "## GRU:\n",
    "Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al. The GRU is like a long short-term memory (LSTM) with a gating mechanism to input or forget certain features, but lacks a context vector or output gate, resulting in fewer parameters than LSTM. GRU's performance on certain tasks of polyphonic music modeling, speech signal modeling and natural language processing was found to be similar to that of LSTM. GRUs showed that gating is indeed helpful in general...\n",
    "(c) https://en.wikipedia.org/wiki/Gated_recurrent_unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM and GRU illustrated guide - https://www.youtube.com/watch?v=8HyCNIVRbSU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
