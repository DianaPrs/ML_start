{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unpacking the black box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![black_box.png](black_box.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put simply, a multilayer perceptron combines together multiple perceptrons into an architecture that has multiple “layers.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.coursera.org/learn/machine-learning?specialization=machine-learning-introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the simplest deep network architectures is a multilayer perceptron with many stacks of hidden layers. A “deep” stack of hidden layers (that’s why we call it “deep” learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "https://playground.tensorflow.org/\n",
    "\n",
    "\n",
    "At its core, an MLP is a feedforward neural network designed for supervised learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "https://medium.com/@tachi.hatim/demystifying-multilayer-perceptrons-the-building-blocks-of-deep-learning-f371f9126609"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "MLPs are trained using the backpropagation algorithm, which computes gradients of a loss function with respect to the model's parameters\n",
    "and updates the parameters iteratively to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Backpropagation is a powerful and efficient algorithm for training MLPs. \n",
    "It involves computing the gradient of the loss function with respect to each weight by propagating the error backward through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "Activation Functions\n",
    "Activation functions are a critical component of MLPs. \n",
    "They introduce non-linearity into the network, allowing it to model complex functions. Without activation functions,\n",
    "an MLP would be equivalent to a single-layer linear model, regardless of the number of layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/epam/Indigo/blob/master/utils/indigo-ml/notebooks/utils/mlp.py"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
